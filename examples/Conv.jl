include("../src/NanoFlux.jl")

# æ˜¾å¼ç‰¹åŒ–é’ˆå¯¹ Conv ç±»å‹çš„ lossï¼Œè¦†ç›– train.jl ä¸­çš„é»˜è®¤é€»è¾‘
function loss(model::Conv, x::AbstractNanoTensor, y::AbstractArray, ps::ParamsType)
    y_pred = model(x, ps) # âš ï¸ å¿…é¡»ä¼ å…¥ ps
    diff = y_pred.data .- y
    L = sum(abs2, diff) / length(y)
    return L
end

# Accuracy å¯¹å›å½’ä»»åŠ¡æ— æ„ä¹‰ï¼Œè¿”å› 0 ä»¥é¿å…æŠ¥é”™
accuracy(model::Conv, x::AbstractNanoTensor, y::AbstractArray, ps::ParamsType) = 0.0

function test_single_conv()
    println("\nğŸ§ª TEST 2: Training a Single Conv Layer (Regression)")
    println("="^60)
    println("â„¹ï¸  Note: Using MSE Loss specifically for Conv layer testing.")

    H, W = 10, 10
    C_in, C_out = 1, 4
    K = 3
    # Conv é…ç½®: Kernel=3, Stride=1, Dilation=1 => OutSize = 10 - 3 + 1 = 8
    Out_H, Out_W = 8, 8
    
    # (Channel, H, W, Batch)
    X_raw = randn(Float32, 1, H, W, 100) 
    Y_target = randn(Float32, C_out, Out_H, Out_W, 100) 
    
    loader = DataLoader((X_raw, Y_target), batchsize=10, shuffle=true)

    model = Conv(2, C_in, C_out, K; act=identity)

    opt = Adam(learning_rate=1e-2) 
    config = TrainerConfig(epochs=20, show_times=5) # å¢åŠ  epochs ç¡®ä¿æ‹Ÿåˆ
    
    train!(model, loader, opt, config)
    
    println("\n", bg"âœ… Single Conv Layer Test Passed!")
end

test_single_conv()