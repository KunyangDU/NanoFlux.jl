# NanoFlux.jl
> **从第一性原理出发基于 NNlib.jl 与 Zygote.jl 构建的纯 Julia 显式梯度深度学习框架。**
## 核心特性
* **纯粹的函数式范式 (Purely Functional Paradigm)**:
    采用显式梯度与无状态设计。模型结构仅承载超参数，权重 被剥离为独立的参数树，彻底解耦计算与状态，完美契合 Zygote.jl 的自动微分机制。

* **原生动态图与即时执行 (Native Dynamic Graph & Define-by-Run)**:
    依托 Zygote.jl 强大的源码级自动微分能力，完美支持 Julia 原生控制流。告别静态图的 DSL 束缚

* **严格的维度安全 (Strict Dimensional Safety)**：
    通过类型系统强制区分“空间数据”与“特征向量”。

* **现代架构原生支持 (Transformer-Native)**：
    内置 MultiHeadAttention, LayerNorm, CausalMask 及 Block 结构，原生支持 GPT 系列模型的构建与训练。
## 支持组件
- 核心张量：`SpatialTensor{N}`, `FlatTensor`
- 网络层：`Dense`, `Conv{N}`, `Pool{N} `(Mean/Max), `Flatten`, `Attension`, `Position`, `LayerNorm`
- 容器：`Sequential`, `Block`
- 优化器：`Adam`, `SGD`
## 性能基准
| 模型架构 | 数据集 | Accuracy | Loss | 训练耗时 | 硬件 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **LeNet-5** | MNIST | **> 98.0%** | **< 0.1** | ~ 12s | Apple M4 (10-Core) |
| **nanoGPT** | shakespeare | **> 60.0%** | **< 1.5** | ~ 1000s | Apple M4 (10-Core) |

### LeNet-5
```
====================================================================================================
Model Inspector
----------------------------------------------------------------------------------------------------
Layer (Type)                                  Input                  Output                 Param #   
----------------------------------------------------------------------------------------------------
Model (Sequential)                            (28, 28, 1)            (10,)                            
├─ Layer 1 (Conv)                             (28, 28, 1)            (24, 24, 6)            156       
├─ Layer 2 (Pool)                             (24, 24, 6)            (12, 12, 6)                      
├─ Layer 3 (Conv)                             (12, 12, 6)            (8, 8, 16)             2,416     
├─ Layer 4 (Pool)                             (8, 8, 16)             (4, 4, 16)                       
├─ Layer 5 (Flatten)                          (4, 4, 16)             (256,)                           
├─ Layer 6 (Dense)                            (256,)                 (120,)                 30,840    
├─ Layer 7 (Dense)                            (120,)                 (84,)                  10,164    
└─ Layer 8 (Dense)                            (84,)                  (10,)                  850       
----------------------------------------------------------------------------------------------------
Total Parameters: 44,426
====================================================================================================
```
### nanoGPT
```
====================================================================================================
Model Inspector
----------------------------------------------------------------------------------------------------
Layer (Type)                                  Input                  Output                 Param #   
----------------------------------------------------------------------------------------------------
Model (Sequential)                            (128,)                 (65, 128)                        
├─ Layer 1 (Embed)                            (128,)                 (128, 128)             8,320     
├─ Layer 2 (Position)                         (128, 128)             (128, 128)             16,384    
├─ Layer 3 (Block)                            (128, 128)             (128, 128)                       
│  ├─ LN1 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  ├─ Attention (Attention)                   (128, 128)             (128, 128)             65,536    
│  ├─ LN2 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  └─ MLP (Sequential)                        (128, 128)             (128, 128)                       
│     ├─ Layer 1 (Dense)                      (128, 128)             (512, 128)             66,048    
│     └─ Layer 2 (Dense)                      (512, 128)             (128, 128)             65,664    
├─ Layer 4 (Block)                            (128, 128)             (128, 128)                       
│  ├─ LN1 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  ├─ Attention (Attention)                   (128, 128)             (128, 128)             65,536    
│  ├─ LN2 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  └─ MLP (Sequential)                        (128, 128)             (128, 128)                       
│     ├─ Layer 1 (Dense)                      (128, 128)             (512, 128)             66,048    
│     └─ Layer 2 (Dense)                      (512, 128)             (128, 128)             65,664    
├─ Layer 5 (Block)                            (128, 128)             (128, 128)                       
│  ├─ LN1 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  ├─ Attention (Attention)                   (128, 128)             (128, 128)             65,536    
│  ├─ LN2 (LayerNorm)                         (128, 128)             (128, 128)             256       
│  └─ MLP (Sequential)                        (128, 128)             (128, 128)                       
│     ├─ Layer 1 (Dense)                      (128, 128)             (512, 128)             66,048    
│     └─ Layer 2 (Dense)                      (512, 128)             (128, 128)             65,664    
├─ Layer 6 (LayerNorm)                        (128, 128)             (128, 128)             256       
└─ Layer 7 (Dense)                            (128, 128)             (65, 128)              8,385     
----------------------------------------------------------------------------------------------------
Total Parameters: 626,625
====================================================================================================
```